# -*- coding: utf-8 -*-
"""roc-consitution.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CYNmNegyaav2LqLu40QeTTF1Pt_nEIbs
"""

# Set environment variable to disable tokenizers parallelism before imports
import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"

import json
import weaviate
from weaviate.classes.init import Auth
from sentence_transformers import SentenceTransformer
from weaviate.util import generate_uuid5
import weaviate.classes.config as wc
from dotenv import load_dotenv

# Cache for sentence transformer model - load only once
_model_cache = None

def get_model():
    """Get or create the sentence transformer model."""
    global _model_cache
    if _model_cache is None:
        _model_cache = SentenceTransformer("BAAI/bge-m3")
    return _model_cache

load_dotenv()
WEAVIATE_URL = os.getenv("WEAVIATE_URL")
WEAVIATE_API_KEY = os.getenv("WEAVIATE_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
assert WEAVIATE_URL, "WEAVIATE_URL is not set"
assert WEAVIATE_API_KEY, "WEAVIATE_API_KEY is not set"
assert OPENAI_API_KEY, "OPENAI_API_KEY is not set"

client = weaviate.connect_to_weaviate_cloud(
    WEAVIATE_URL,
    auth_credentials=Auth.api_key(WEAVIATE_API_KEY),
    headers={
        "X-OpenAI-Api-Key": OPENAI_API_KEY
    }
)
model = get_model()

assert client.is_ready(), "Weaviate client is not ready. Check credentials and endpoint."

print(f"weaviate is ready? {client.is_ready()}")

try:
    collection = client.collections.get("ROC_Constitution_BG3_M3")
    print(f"Collection {collection.name} already exists.")
except Exception as e:
    print(f"Collection doesn't exist. Creating a new one. Error: {e}")
    # Create collection
    collection = client.collections.create(
        name="ROC_Constitution_BG3_M3",
        vectorizer_config=None,
        properties=[
            wc.Property(name="content", data_type=wc.DataType.TEXT),
            wc.Property(name="title", data_type=wc.DataType.TEXT),
            wc.Property(name="slug", data_type=wc.DataType.TEXT),
            wc.Property(name="section", data_type=wc.DataType.TEXT),
            wc.Property(name="chapter", data_type=wc.DataType.TEXT),
            wc.Property(name="article", data_type=wc.DataType.TEXT),
            wc.Property(name="year", data_type=wc.DataType.INT),
        ]
    )

# Function to batch upload to Weaviate (including UUIDs)
def bulk_upload_space_chunks_to_weaviate(json_file_path, collection_name="ROC_Constitution_BG3_M3", skip_existing=True, update_existing=False):

    docs_collection = client.collections.get(collection_name)

    with open(json_file_path, "r", encoding="utf-8") as f:
        chunks = json.load(f)

    successful_uploads = 0
    skipped_existing = 0
    updated_existing = 0
    
    # Use the cached model
    model = get_model()

    with docs_collection.batch.fixed_size(batch_size=50, concurrent_requests=2) as batch:
        for i, chunk in enumerate(chunks):
            text = chunk.get("text", "")
            metadata = chunk.get("metadata", {})

            # Create a unique ID by combining title with chunk index and first 20 chars of text
            unique_id = f"{metadata.get('title', 'unknown')}-chunk-{i}-{text[:20]}"
            uid = generate_uuid5(unique_id)

            # Check if object with this UUID already exists
            try:
                existing_object = docs_collection.query.fetch_object_by_id(uid)
                if existing_object and skip_existing and not update_existing:
                    skipped_existing += 1
                    if i % 500 == 0 and i > 0:
                        print(f"Progress: {i}/{len(chunks)} chunks processed")
                    continue
                elif existing_object and update_existing:
                    # Update the existing object
                    docs_collection.data.update(
                        uuid=uid,
                        properties={
                            "content": text,
                            "title": metadata.get("title", "unknown"),
                            "slug": metadata.get("slug", "unknown"),
                            "section": metadata.get("section", None),
                            "chapter": metadata.get("chapter", None),
                            "article": metadata.get("article", None),
                            "year": metadata.get("year", None)
                        },
                        vector=model.encode(text, normalize_embeddings=True)
                    )
                    updated_existing += 1
                    if i % 500 == 0 and i > 0:
                        print(f"Progress: {i}/{len(chunks)} chunks processed")
                    continue
            except Exception as e:
                # Object doesn't exist or error checking, proceed with adding
                pass

            # Add new object
            batch.add_object(
                properties={
                    "content": text,
                    "title": metadata.get("title", "unknown"),
                    "slug": metadata.get("slug", "unknown"),
                    "section": metadata.get("section", None),
                    "chapter": metadata.get("chapter", None),
                    "article": metadata.get("article", None),
                    "year": metadata.get("year", None)
                },
                uuid=uid,
                vector=model.encode(text, normalize_embeddings=True)
            )
            successful_uploads += 1

            # Progress indicator
            if i % 500 == 0 and i > 0:
                print(f"Progress: {i}/{len(chunks)} chunks processed")

            if batch.number_errors > 10:
                print("❌ Too many errors during batch import — stopping early.")
                break

    # Verify the actual count in the collection
    collection_count = docs_collection.aggregate.over_all().total_count
    print(f"✅ Uploaded {successful_uploads} new chunks to Weaviate from {json_file_path}")
    print(f"✅ Skipped {skipped_existing} existing chunks")
    if update_existing:
        print(f"✅ Updated {updated_existing} existing chunks")
    print(f"✅ Collection now contains {collection_count} objects")

# Update the function call with appropriate parameters
# Set skip_existing=True to skip chunks that already exist
# Set update_existing=True to update existing chunks with new data
bulk_upload_space_chunks_to_weaviate("chunks.json", skip_existing=True, update_existing=False)
client.close()